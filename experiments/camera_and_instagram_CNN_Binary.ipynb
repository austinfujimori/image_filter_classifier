{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e935cb-52c5-4717-a344-86dd4af935b2",
   "metadata": {},
   "source": [
    "## This model is the binary classification version of the camera_and_instagram_CNN_Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9ab33-ba62-4791-940d-45d6e59e753b",
   "metadata": {},
   "source": [
    "The purpose of this was to see that if reducing the classes from 16 to 2, which reduced the parameter count greatly, would be more effective when tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfd17c-bbba-4a42-a997-2f98a56f5543",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d9e8ea-73e0-4ed0-9ac0-8cdede5ca074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "from IPython.display import Image as ImagePy\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2493-04e0-4795-9218-f4570451caf8",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "Our current dataset folder looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2eeea0e-79e5-4517-a327-ec520fb21506",
   "metadata": {},
   "source": [
    "dataset\n",
    "\n",
    " --- unfiltered\n",
    "       --- train\n",
    "             --- images\n",
    "       --- val\n",
    "             --- images\n",
    "\n",
    " --- filtered\n",
    "       --- train\n",
    "             --- filter folders\n",
    "                   --- images\n",
    "       --- val\n",
    "             --- filter folders\n",
    "                   --- images\n",
    "\n",
    " --- camera_filtered\n",
    "       --- train\n",
    "             --- filter folders\n",
    "                   --- images\n",
    "       --- val\n",
    "             --- filter folders\n",
    "                   --- images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d03240-8332-4e7a-818a-63b1a308b7e7",
   "metadata": {},
   "source": [
    "# Count images in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67aaa03c-348b-4a49-91e5-256a20627bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(folder):\n",
    "    files = os.listdir(folder)\n",
    "    images = [file for file in files if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    return len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af74582-66d0-4e36-af2f-c67a17426356",
   "metadata": {},
   "source": [
    "#### For our unfiltered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "443e778d-fc52-47bf-8a3f-7bf538628ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train unfiltered images: 86744\n",
      "Number of val unfiltered images: 10954\n"
     ]
    }
   ],
   "source": [
    "unfiltered_folder = \"dataset/unfiltered\"\n",
    "\n",
    "for filter_name in os.listdir(unfiltered_folder):\n",
    "    filter_folder = os.path.join(unfiltered_folder, filter_name)\n",
    "    if os.path.isdir(filter_folder):\n",
    "        num_filtered_images = count_images(filter_folder)\n",
    "        print(f'Number of {filter_name} unfiltered images: {num_filtered_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef3dfe-ba1f-43e7-8c16-13f7b1daf003",
   "metadata": {},
   "source": [
    "#### For our camera_filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "063a4b3c-107e-40a7-b9a1-56111e6cec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of brightness images in train: 40000\n",
      "Number of contrast images in train: 40000\n",
      "Number of sepia images in train: 40000\n",
      "Number of black_and_white images in train: 40000\n",
      "Number of blur images in train: 40000\n",
      "Number of cool images in train: 40000\n",
      "Number of warmth images in train: 40000\n",
      "Number of brightness images in val: 3000\n",
      "Number of contrast images in val: 3000\n",
      "Number of sepia images in val: 3000\n",
      "Number of black_and_white images in val: 3000\n",
      "Number of blur images in val: 3000\n",
      "Number of cool images in val: 3000\n",
      "Number of warmth images in val: 3000\n"
     ]
    }
   ],
   "source": [
    "camera_filtered = 'dataset/camera_filtered'\n",
    "\n",
    "for subdir in ['train', 'val']:\n",
    "    subdir_path = os.path.join(camera_filtered, subdir)\n",
    "    for filter_name in [\"brightness\", \"contrast\", \"sepia\", \"black_and_white\", \"blur\", \"cool\", \"warmth\"]:\n",
    "        filter_dir = os.path.join(subdir_path, filter_name)\n",
    "        num_images = count_images(filter_dir)\n",
    "        print(f\"Number of {filter_name} images in {subdir}: {num_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc8059-fc57-4e92-971c-be73e154d31b",
   "metadata": {},
   "source": [
    "#### For our filtered dataset:\n",
    "\n",
    "The Instagram dataset contains about 10k images per train folder for each filter and 1k for val folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a4a66-6284-4bab-8067-77c62d9282d6",
   "metadata": {},
   "source": [
    "# Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc4c07c0-37ba-49ec-b91c-1023742867a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, base_dir, split='train', transform=None):\n",
    "        self.base_dir = base_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.image_labels = []\n",
    "\n",
    "        unfiltered_dir = os.path.join(base_dir, 'unfiltered', split)\n",
    "        for img_name in os.listdir(unfiltered_dir):\n",
    "            if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                self.image_paths.append(os.path.join(unfiltered_dir, img_name))\n",
    "                self.image_labels.append(0)\n",
    "\n",
    "        self.process_directory('filtered')\n",
    "        self.process_directory('camera_filtered')\n",
    "\n",
    "    def process_directory(self, dir_name):\n",
    "        dir_path = os.path.join(self.base_dir, dir_name, self.split)\n",
    "        for label_dir in os.listdir(dir_path):\n",
    "            full_dir = os.path.join(dir_path, label_dir)\n",
    "            if os.path.isdir(full_dir):\n",
    "                for img_name in os.listdir(full_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_paths.append(os.path.join(full_dir, img_name))\n",
    "                        self.image_labels.append(1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.image_labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = BinaryDataset(base_dir='dataset', split='train', transform=transform)\n",
    "val_dataset = BinaryDataset(base_dir='dataset', split='val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f85c30a1-2796-4b75-9555-8cbcdf29cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 444718 images - 357974 filtered, 86744 unfiltered\n",
      "Validation set: 41760 images - 30806 filtered, 10954 unfiltered\n"
     ]
    }
   ],
   "source": [
    "train_filtered = sum(train_dataset.image_labels)\n",
    "train_unfiltered = len(train_dataset.image_labels) - train_filtered\n",
    "val_filtered = sum(val_dataset.image_labels)\n",
    "val_unfiltered = len(val_dataset.image_labels) - val_filtered\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images - {train_filtered} filtered, {train_unfiltered} unfiltered\")\n",
    "print(f\"Validation set: {len(val_dataset)} images - {val_filtered} filtered, {val_unfiltered} unfiltered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc109db6-e400-4b45-9da1-dd286ea0992c",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b6a9809-9fa6-4e46-a44b-c097d3a6e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, padding='valid')\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding='valid', bias=False)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding='valid', bias=False)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding='valid', bias=False)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.dropout1(self.pool1(self.act1(self.conv1(x)))))\n",
    "        x = self.bn2(self.dropout2(self.pool2(self.act2(self.conv2(x)))))\n",
    "        x = self.bn3(self.dropout3(self.act3(self.conv3(x))))\n",
    "        x = self.bn4(self.dropout4(self.act4(self.conv4(x))))\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        x = torch.flatten(x, 1)\n",
    "                         \n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel(num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b28ce-8d93-4dd0-b978-fe35ad2cfc45",
   "metadata": {},
   "source": [
    "# Evaluate the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cd94d-bc3a-475f-b69f-63e7cef1c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the validation set: {accuracy:.2f}%')\n",
    "\n",
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d933a-42b0-4e62-a5aa-01c37ee58b9a",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55a4fc01-f863-4072-bb23-61114c7810d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.4012, Train Acc: 81.54%, Val Loss: 0.4652, Val Acc: 79.40%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_1.pth\n",
      "Epoch 2/50, Train Loss: 0.3255, Train Acc: 85.80%, Val Loss: 0.4107, Val Acc: 82.47%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_2.pth\n",
      "Epoch 3/50, Train Loss: 0.2853, Train Acc: 88.00%, Val Loss: 0.3555, Val Acc: 85.75%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_3.pth\n",
      "Epoch 4/50, Train Loss: 0.2557, Train Acc: 89.45%, Val Loss: 0.3536, Val Acc: 84.97%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_4.pth\n",
      "Epoch 5/50, Train Loss: 0.2380, Train Acc: 90.32%, Val Loss: 0.3826, Val Acc: 82.80%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_5.pth\n",
      "Epoch 6/50, Train Loss: 0.2244, Train Acc: 90.93%, Val Loss: 0.5220, Val Acc: 71.19%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_6.pth\n",
      "Epoch 7/50, Train Loss: 0.2153, Train Acc: 91.37%, Val Loss: 0.6278, Val Acc: 63.50%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_7.pth\n",
      "Epoch 8/50, Train Loss: 0.2064, Train Acc: 91.80%, Val Loss: 0.8979, Val Acc: 48.51%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_8.pth\n",
      "Epoch 9/50, Train Loss: 0.1999, Train Acc: 92.03%, Val Loss: 0.3290, Val Acc: 86.60%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_9.pth\n",
      "Epoch 10/50, Train Loss: 0.1942, Train Acc: 92.28%, Val Loss: 0.5303, Val Acc: 71.40%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_10.pth\n",
      "Epoch 11/50, Train Loss: 0.1905, Train Acc: 92.44%, Val Loss: 0.5536, Val Acc: 69.73%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_11.pth\n",
      "Epoch 12/50, Train Loss: 0.1869, Train Acc: 92.61%, Val Loss: 0.6311, Val Acc: 64.90%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_12.pth\n",
      "Epoch 13/50, Train Loss: 0.1837, Train Acc: 92.77%, Val Loss: 0.2405, Val Acc: 90.73%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_13.pth\n",
      "Epoch 14/50, Train Loss: 0.1812, Train Acc: 92.88%, Val Loss: 1.0146, Val Acc: 44.52%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_14.pth\n",
      "Epoch 15/50, Train Loss: 0.1774, Train Acc: 93.02%, Val Loss: 0.7012, Val Acc: 59.13%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_15.pth\n",
      "Epoch 16/50, Train Loss: 0.1759, Train Acc: 93.07%, Val Loss: 0.6074, Val Acc: 64.89%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_16.pth\n",
      "Epoch 17/50, Train Loss: 0.1728, Train Acc: 93.21%, Val Loss: 0.5248, Val Acc: 70.99%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_17.pth\n",
      "Epoch 18/50, Train Loss: 0.1719, Train Acc: 93.21%, Val Loss: 0.2878, Val Acc: 87.74%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_18.pth\n",
      "Epoch 19/50, Train Loss: 0.1701, Train Acc: 93.30%, Val Loss: 0.5354, Val Acc: 71.48%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_19.pth\n",
      "Epoch 20/50, Train Loss: 0.1681, Train Acc: 93.41%, Val Loss: 0.4566, Val Acc: 75.36%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_20.pth\n",
      "Epoch 21/50, Train Loss: 0.1674, Train Acc: 93.45%, Val Loss: 0.5584, Val Acc: 68.66%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_21.pth\n",
      "Epoch 22/50, Train Loss: 0.1646, Train Acc: 93.50%, Val Loss: 0.5073, Val Acc: 72.11%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_22.pth\n",
      "Epoch 23/50, Train Loss: 0.1644, Train Acc: 93.53%, Val Loss: 0.2476, Val Acc: 90.12%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_23.pth\n",
      "Epoch 24/50, Train Loss: 0.1621, Train Acc: 93.61%, Val Loss: 0.5632, Val Acc: 68.69%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_24.pth\n",
      "Epoch 25/50, Train Loss: 0.1615, Train Acc: 93.63%, Val Loss: 0.2394, Val Acc: 90.91%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_25.pth\n",
      "Epoch 26/50, Train Loss: 0.1593, Train Acc: 93.69%, Val Loss: 0.4718, Val Acc: 75.18%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_26.pth\n",
      "Epoch 27/50, Train Loss: 0.1587, Train Acc: 93.77%, Val Loss: 0.3809, Val Acc: 81.46%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_27.pth\n",
      "Epoch 28/50, Train Loss: 0.1576, Train Acc: 93.80%, Val Loss: 0.8742, Val Acc: 51.02%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_28.pth\n",
      "Epoch 29/50, Train Loss: 0.1555, Train Acc: 93.90%, Val Loss: 0.7229, Val Acc: 61.15%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_29.pth\n",
      "Epoch 30/50, Train Loss: 0.1549, Train Acc: 93.94%, Val Loss: 0.6430, Val Acc: 66.35%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_30.pth\n",
      "Epoch 31/50, Train Loss: 0.1527, Train Acc: 94.06%, Val Loss: 0.7127, Val Acc: 61.04%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_31.pth\n",
      "Epoch 32/50, Train Loss: 0.1525, Train Acc: 94.02%, Val Loss: 0.4072, Val Acc: 79.68%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_32.pth\n",
      "Epoch 33/50, Train Loss: 0.1517, Train Acc: 94.08%, Val Loss: 1.2164, Val Acc: 42.23%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_33.pth\n",
      "Epoch 34/50, Train Loss: 0.1507, Train Acc: 94.18%, Val Loss: 0.5764, Val Acc: 68.35%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_34.pth\n",
      "Epoch 35/50, Train Loss: 0.1500, Train Acc: 94.13%, Val Loss: 0.4435, Val Acc: 77.28%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_35.pth\n",
      "Epoch 36/50, Train Loss: 0.1495, Train Acc: 94.21%, Val Loss: 0.4942, Val Acc: 73.82%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_36.pth\n",
      "Epoch 37/50, Train Loss: 0.1485, Train Acc: 94.22%, Val Loss: 0.7896, Val Acc: 58.51%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_37.pth\n",
      "Epoch 38/50, Train Loss: 0.1474, Train Acc: 94.27%, Val Loss: 0.4275, Val Acc: 78.53%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_38.pth\n",
      "Epoch 39/50, Train Loss: 0.1470, Train Acc: 94.31%, Val Loss: 1.1353, Val Acc: 43.84%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_39.pth\n",
      "Epoch 40/50, Train Loss: 0.1465, Train Acc: 94.32%, Val Loss: 0.7451, Val Acc: 59.66%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_40.pth\n",
      "Epoch 41/50, Train Loss: 0.1458, Train Acc: 94.39%, Val Loss: 0.9256, Val Acc: 50.75%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_41.pth\n",
      "Epoch 42/50, Train Loss: 0.1446, Train Acc: 94.41%, Val Loss: 0.2895, Val Acc: 87.45%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_42.pth\n",
      "Epoch 43/50, Train Loss: 0.1438, Train Acc: 94.44%, Val Loss: 1.0953, Val Acc: 45.10%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_43.pth\n",
      "Epoch 44/50, Train Loss: 0.1443, Train Acc: 94.46%, Val Loss: 0.6356, Val Acc: 65.50%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_44.pth\n",
      "Epoch 45/50, Train Loss: 0.1433, Train Acc: 94.45%, Val Loss: 0.6807, Val Acc: 62.97%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_45.pth\n",
      "Epoch 46/50, Train Loss: 0.1423, Train Acc: 94.48%, Val Loss: 0.5627, Val Acc: 70.54%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_46.pth\n",
      "Epoch 47/50, Train Loss: 0.1427, Train Acc: 94.53%, Val Loss: 0.5936, Val Acc: 68.23%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_47.pth\n",
      "Epoch 48/50, Train Loss: 0.1419, Train Acc: 94.54%, Val Loss: 0.6083, Val Acc: 66.42%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_48.pth\n",
      "Epoch 49/50, Train Loss: 0.1414, Train Acc: 94.57%, Val Loss: 0.5612, Val Acc: 70.25%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_49.pth\n",
      "Epoch 50/50, Train Loss: 0.1413, Train Acc: 94.56%, Val Loss: 0.7346, Val Acc: 61.01%\n",
      "Checkpoint saved to checkpoints/checkpoint_epoch_50.pth\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = 'checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_preds += labels.size(0)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_accuracy = 100 * correct_preds / total_preds\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total_preds += labels.size(0)\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct_preds / val_total_preds\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70979f-415e-4854-b159-56943c39bf86",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5623c-4a09-4c44-b3ca-16ad3825f12b",
   "metadata": {},
   "source": [
    "Didn't do well on test data. Definitely overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429da772-f1c4-4726-9834-d0c5c10bdb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
